{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.spatial.distance import cityblock, chebyshev, minkowski, cosine, mahalanobis, euclidean\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from local_utils import init_embeddings\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from scipy.special import betainc\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Constants for file paths\n",
    "EMBEDDINGS_PATH = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/model_240000_DoppelVer_All_112x112_outputs.pth\"\n",
    "EMBEDDINGS_IMAGE_PATH = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/model_240000_DoppelVer_All_112x112_image_paths.txt\"\n",
    "IMAGE_DIR = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/DoppelVer_All_112x112\"\n",
    "BASE_PATH = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/00hypersphere_Comparison/results\"\n",
    "CLASS_NUM_NAME_PATH = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/class_num_name.csv\"\n",
    "\n",
    "embeddings = init_embeddings(EMBEDDINGS_PATH, EMBEDDINGS_IMAGE_PATH, IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_per_person(embeddings, base_path, n_clusters=1):\n",
    "    # Ensure the base directory exists\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    filename = os.path.join(base_path, \"KMeans_Clustering_Per_Person.csv\")\n",
    "\n",
    "    # Exclude the last 3 columns: 'class', 'path', and 'class_num'\n",
    "    embedding_columns = embeddings.columns[:-3]\n",
    "\n",
    "    for person_index in tqdm(embedding_columns, desc=\"Clustering Each Person\"):\n",
    "        # Extract and scale data for the current person (column)\n",
    "        person_data = embeddings[person_index].values.reshape(-1, 1)\n",
    "        person_data_scaled = StandardScaler().fit_transform(person_data)\n",
    "        \n",
    "        # Apply KMeans clustering\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = clusterer.fit_predict(person_data_scaled)\n",
    "        \n",
    "        # Collect results\n",
    "        for idx, label in enumerate(labels):\n",
    "            results.append({\n",
    "                'person_index': person_index,\n",
    "                'data_point_index': idx,\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "    # Save all results at once to minimize file I/O operations\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_find_midpoint_and_furthest_distance_per_person(embeddings, base_path, class_num_name_df):\n",
    "    base_path = os.path.join(base_path, \"Chebyshev\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    midpoints = []\n",
    "    filename = os.path.join(base_path, \"Chebyshev_Midpoints_Per_Person_With_Class_Names.csv\")\n",
    "\n",
    "    for person_index in tqdm(embeddings['class_num'].unique(), desc=\"Calculating Midpoints for Each Person\"):\n",
    "        # Extract data for the current person (class_num)\n",
    "        person_data = embeddings[embeddings['class_num'] == person_index].iloc[:, :-3].values\n",
    "\n",
    "        # Calculate the midpoint (mean) of the cluster\n",
    "        midpoint = np.mean(person_data, axis=0)\n",
    "        \n",
    "        # Calculate the Chebyshev distance from the midpoint to each point in the cluster\n",
    "        distances = np.array([chebyshev(midpoint, point) for point in person_data])\n",
    "        \n",
    "        # Find the maximum distance (furthest point)\n",
    "        max_distance = np.max(distances)\n",
    "        \n",
    "        # Get the corresponding class_num and class_name\n",
    "        class_name_row = class_num_name_df[class_num_name_df['class_num'] == person_index]\n",
    "\n",
    "        if class_name_row.empty:\n",
    "            continue\n",
    "\n",
    "        class_name = class_name_row['class'].values[0]\n",
    "\n",
    "        # Format the midpoint array as a string with commas\n",
    "        midpoint_str = \", \".join(map(str, midpoint))\n",
    "        \n",
    "        # Collect results\n",
    "        midpoints.append({\n",
    "            'person_index': person_index,\n",
    "            'class_num': person_index,\n",
    "            'class': class_name,\n",
    "            'midpoint': midpoint_str,\n",
    "            'max_distance': max_distance\n",
    "        })\n",
    "\n",
    "    # Convert midpoints to DataFrame\n",
    "    midpoints_df = pd.DataFrame(midpoints)\n",
    "    \n",
    "    # Save all results at once to minimize file I/O operations\n",
    "    midpoints_df.to_csv(filename, index=False)\n",
    "    \n",
    "    return midpoints_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_find_midpoint_and_furthest_distance_per_person(embeddings, base_path, class_num_name_df, p=2):\n",
    "    base_path = os.path.join(base_path, \"Minkowski\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    midpoints = []\n",
    "    filename = os.path.join(base_path, \"Minkowski_Midpoints_Per_Person_With_Class_Names.csv\")\n",
    "\n",
    "    for person_index in tqdm(embeddings['class_num'].unique(), desc=\"Calculating Midpoints for Each Person\"):\n",
    "        # Extract data for the current person (class_num)\n",
    "        person_data = embeddings[embeddings['class_num'] == person_index].iloc[:, :-3].values\n",
    "        \n",
    "        # Calculate the midpoint (mean) of the cluster\n",
    "        midpoint = np.mean(person_data, axis=0)\n",
    "        \n",
    "        # Calculate the Minkowski distance from the midpoint to each point in the cluster\n",
    "        distances = np.array([minkowski(midpoint, point, p) for point in person_data])\n",
    "        \n",
    "        # Find the maximum distance (furthest point)\n",
    "        max_distance = np.max(distances)\n",
    "        \n",
    "        # Get the corresponding class_num and class_name\n",
    "        class_name_row = class_num_name_df[class_num_name_df['class_num'] == person_index]\n",
    "\n",
    "        if class_name_row.empty:\n",
    "            continue\n",
    "\n",
    "        class_name = class_name_row['class'].values[0]\n",
    "\n",
    "        # Format the midpoint array as a string with commas\n",
    "        midpoint_str = \", \".join(map(str, midpoint))\n",
    "        \n",
    "        # Collect results\n",
    "        midpoints.append({\n",
    "            'person_index': person_index,\n",
    "            'class_num': person_index,\n",
    "            'class': class_name,\n",
    "            'midpoint': midpoint_str,\n",
    "            'max_distance': max_distance\n",
    "        })\n",
    "\n",
    "    # Convert midpoints to DataFrame\n",
    "    midpoints_df = pd.DataFrame(midpoints)\n",
    "    \n",
    "    # Save all results at once to minimize file I/O operations\n",
    "    midpoints_df.to_csv(filename, index=False)\n",
    "    \n",
    "    return midpoints_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_find_midpoint_and_furthest_distance_per_person(embeddings, base_path, class_num_name_df):\n",
    "    base_path = os.path.join(base_path, \"Cosine\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    midpoints = []\n",
    "    filename = os.path.join(base_path, \"Cosine_Midpoints_Per_Person_With_Class_Names.csv\")\n",
    "\n",
    "    for person_index in tqdm(embeddings['class_num'].unique(), desc=\"Calculating Midpoints for Each Person\"):\n",
    "        # Extract data for the current person (class_num)\n",
    "        person_data = embeddings[embeddings['class_num'] == person_index].iloc[:, :-3].values\n",
    "        \n",
    "        # Calculate the midpoint (mean) of the cluster\n",
    "        midpoint = np.mean(person_data, axis=0)\n",
    "        \n",
    "        # Calculate the Cosine distance from the midpoint to each point in the cluster\n",
    "        distances = np.array([cosine(midpoint, point) for point in person_data])\n",
    "        \n",
    "        # Find the maximum distance (furthest point)\n",
    "        max_distance = np.max(distances)\n",
    "        \n",
    "        # Get the corresponding class_num and class_name\n",
    "        class_name_row = class_num_name_df[class_num_name_df['class_num'] == person_index]\n",
    "\n",
    "        if class_name_row.empty:\n",
    "            continue\n",
    "\n",
    "        class_name = class_name_row['class'].values[0]\n",
    "\n",
    "        # Format the midpoint array as a string with commas\n",
    "        midpoint_str = \", \".join(map(str, midpoint))\n",
    "        \n",
    "        # Collect results\n",
    "        midpoints.append({\n",
    "            'person_index': person_index,\n",
    "            'class_num': person_index,\n",
    "            'class': class_name,\n",
    "            'midpoint': midpoint_str,\n",
    "            'max_distance': max_distance\n",
    "        })\n",
    "\n",
    "    # Convert midpoints to DataFrame\n",
    "    midpoints_df = pd.DataFrame(midpoints)\n",
    "    \n",
    "    # Save all results at once to minimize file I/O operations\n",
    "    midpoints_df.to_csv(filename, index=False)\n",
    "    \n",
    "    return midpoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(base_path):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# Read midpoints and max distances from CSV\n",
    "def read_csv(midpoints_csv_path):\n",
    "    return pd.read_csv(midpoints_csv_path)\n",
    "\n",
    "# Function to create hyperspheres and calculate overlap\n",
    "def create_hyperspheres_and_calculate_overlap_from_csv(midpoints_csv_path, overlap_output_base_path):\n",
    "    create_directory(overlap_output_base_path)\n",
    "    \n",
    "    # Read midpoints and max distances from CSV\n",
    "    midpoints_df = read_csv(midpoints_csv_path)\n",
    "    \n",
    "    hyperspheres = []\n",
    "    overlap_results = []\n",
    "    overlap_filename = os.path.join(overlap_output_base_path, \"Hypersphere_Overlap_Per_Person.csv\")\n",
    "\n",
    "    # Loop over each row in the midpoints DataFrame\n",
    "    for index, row in tqdm(midpoints_df.iterrows(), total=midpoints_df.shape[0], desc=\"Calculating Hyperspheres for Each Person\"):\n",
    "        # Extract data from the row\n",
    "        person_index = row['person_index']\n",
    "        class_num = row['class_num']\n",
    "        class_name = row['class']\n",
    "        midpoint_str = row['midpoint']\n",
    "        midpoint = np.array([float(x.strip()) for x in midpoint_str.split(',')])\n",
    "        max_distance = row['max_distance']\n",
    "        \n",
    "        # Collect results\n",
    "        hyperspheres.append({\n",
    "            'person_index': person_index,\n",
    "            'class_num': class_num,\n",
    "            'class': class_name,\n",
    "            'midpoint': midpoint,\n",
    "            'radius': max_distance  # Radius is the same as max_distance\n",
    "        })\n",
    "\n",
    "    # Calculate overlap with other hyperspheres\n",
    "    for i in range(len(hyperspheres)):\n",
    "        for j in range(i + 1, len(hyperspheres)):\n",
    "            hypersphere_1 = hyperspheres[i]\n",
    "            hypersphere_2 = hyperspheres[j]\n",
    "            \n",
    "            # Calculate distance between midpoints using Euclidean distance\n",
    "            distance_between_centers = euclidean(hypersphere_1['midpoint'], hypersphere_2['midpoint'])\n",
    "            \n",
    "            # Check for overlap\n",
    "            if distance_between_centers < (hypersphere_1['radius'] + hypersphere_2['radius']):\n",
    "                overlap_percentage = (1 - distance_between_centers / (hypersphere_1['radius'] + hypersphere_2['radius'])) * 100\n",
    "                overlap_results.append({\n",
    "                    'person_index_1': hypersphere_1['person_index'],\n",
    "                    'class_1': hypersphere_1['class'],\n",
    "                    'person_index_2': hypersphere_2['person_index'],\n",
    "                    'class_2': hypersphere_2['class'],\n",
    "                    'overlap_percentage': overlap_percentage\n",
    "                })\n",
    "\n",
    "    # Convert overlap results to DataFrame\n",
    "    overlap_df = pd.DataFrame(overlap_results)\n",
    "    \n",
    "    # Save overlap results to CSV\n",
    "    overlap_df.to_csv(overlap_filename, index=False)\n",
    "    \n",
    "    return overlap_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_overlap_csv(input_csv_path, output_csv_path):\n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Sort the DataFrame by 'overlap_percentage' column\n",
    "    sorted_df = df.sort_values(by='overlap_percentage', ascending=True)\n",
    "    \n",
    "    # Save the sorted DataFrame to the output CSV file\n",
    "    sorted_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Sorted data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Midpoints for Each Person: 100%|██████████| 390/390 [00:00<00:00, 663.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load class_num_name_df\n",
    "class_num_name_df = pd.read_csv(CLASS_NUM_NAME_PATH)\n",
    "\n",
    "# Run the Cosine function to find midpoints and furthest distances\n",
    "midpoints_df = cosine_find_midpoint_and_furthest_distance_per_person(embeddings, BASE_PATH, class_num_name_df)\n",
    "\n",
    "# Define paths for midpoints CSV and overlap output directory\n",
    "midpoints_csv_path = os.path.join(BASE_PATH, \"Cosine/Cosine_Midpoints_Per_Person_With_Class_Names.csv\")\n",
    "overlap_output_base_path = os.path.join(BASE_PATH, \"Cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_hyperspheres_and_calculate_overlap_from_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m overlap_output_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/00hypersphere_Comparison/results/Cosine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate overlaps and save to CSV\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m overlap_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_hyperspheres_and_calculate_overlap_from_csv\u001b[49m(midpoints_csv_path, overlap_output_base_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_hyperspheres_and_calculate_overlap_from_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Paths for the input CSV and output CSV\n",
    "input_csv_path = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/00hypersphere_Comparison/results/Cosine/Hypersphere_Overlap_Per_Person.csv\"\n",
    "output_csv_path = \"/home/nmichelotti/Desktop/Embeddings/embeddings_for_n8/00hypersphere_Comparison/results/Cosine/Sorted_Hypersphere_Overlap_Per_Person.csv\"\n",
    "\n",
    "# Sort the CSV by overlap percentage and save to a new file\n",
    "sort_overlap_csv(input_csv_path, output_csv_path)\n",
    "\n",
    "# Print the first few rows of the sorted result to verify\n",
    "sorted_df = pd.read_csv(output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
